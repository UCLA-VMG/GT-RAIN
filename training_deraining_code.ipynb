{"cells":[{"cell_type":"markdown","metadata":{"id":"RGdsboBYrsPr"},"source":["# Library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wb9trfBUHTnE"},"outputs":[],"source":["import sys\n","import os\n","\n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms.functional as TF\n","from torch.autograd import Variable\n","import functools\n","from torch.nn import init\n","import torchvision\n","\n","from PIL import Image, ImageChops, ImageOps, ImageEnhance\n","import random\n","from natsort import natsorted\n","from glob import glob\n","\n","from tqdm.notebook import tqdm\n","from pathlib import Path\n","import time\n","import numpy as np\n","from tensorboardX import SummaryWriter\n","%load_ext tensorboard\n","from skimage import img_as_ubyte\n","import cv2\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from math import exp\n","from piq import MultiScaleSSIMLoss\n","import math\n","import copy"]},{"cell_type":"markdown","metadata":{"id":"ddumLfsWtvnA"},"source":["# Helper\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iCmCyZwitvvM"},"outputs":[],"source":["# Plot image\n","def show_img(img, title=None, figsize=(15, 15)):\n","  #img = (img+1)/2\n","  plt.figure(figsize=figsize)\n","  plt.imshow(img)\n","  if title:\n","    plt.title(title)\n","  plt.show()"]},{"cell_type":"markdown","source":["# Data Augmentation"],"metadata":{"id":"uMye_Kcrk2vd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"D33mj2AmQmYn"},"outputs":[],"source":["# ROTATION DATA AUGMENTATION CODE\n","# Code modified from https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders\n","\n","def get_translation_matrix_2d(dx, dy):\n","  \"\"\"\n","  Returns a numpy affine transformation matrix for a 2D translation of\n","  (dx, dy)\n","  \"\"\"\n","  return np.matrix([[1, 0, dx], [0, 1, dy], [0, 0, 1]])\n","\n","def rotate_image(image, angle):\n","  \"\"\"\n","  Rotates the given image about it's centre\n","  \"\"\"\n","\n","  image_size = (image.shape[1], image.shape[0])\n","  image_center = tuple(np.array(image_size) / 2)\n","\n","  rot_mat = np.vstack([cv2.getRotationMatrix2D(image_center, angle, 1.0), [0, 0, 1]])\n","  trans_mat = np.identity(3)\n","\n","  w2 = image_size[0] * 0.5\n","  h2 = image_size[1] * 0.5\n","\n","  rot_mat_notranslate = np.matrix(rot_mat[0:2, 0:2])\n","\n","  tl = (np.array([-w2, h2]) * rot_mat_notranslate).A[0]\n","  tr = (np.array([w2, h2]) * rot_mat_notranslate).A[0]\n","  bl = (np.array([-w2, -h2]) * rot_mat_notranslate).A[0]\n","  br = (np.array([w2, -h2]) * rot_mat_notranslate).A[0]\n","\n","  x_coords = [pt[0] for pt in [tl, tr, bl, br]]\n","  x_pos = [x for x in x_coords if x > 0]\n","  x_neg = [x for x in x_coords if x < 0]\n","\n","  y_coords = [pt[1] for pt in [tl, tr, bl, br]]\n","  y_pos = [y for y in y_coords if y > 0]\n","  y_neg = [y for y in y_coords if y < 0]\n","\n","  right_bound = max(x_pos)\n","  left_bound = min(x_neg)\n","  top_bound = max(y_pos)\n","  bot_bound = min(y_neg)\n","\n","  new_w = int(abs(right_bound - left_bound))\n","  new_h = int(abs(top_bound - bot_bound))\n","  new_image_size = (new_w, new_h)\n","\n","  new_midx = new_w * 0.5\n","  new_midy = new_h * 0.5\n","\n","  dx = int(new_midx - w2)\n","  dy = int(new_midy - h2)\n","\n","  trans_mat = get_translation_matrix_2d(dx, dy)\n","  affine_mat = (np.matrix(trans_mat) * np.matrix(rot_mat))[0:2, :]\n","  result = cv2.warpAffine(image, affine_mat, new_image_size, flags=cv2.INTER_LINEAR)\n","\n","  return result\n","\n","def rotated_rect_with_max_area(w, h, angle):\n","  \"\"\"\n","  Given a rectangle of size wxh that has been rotated by 'angle' (in\n","  radians), computes the width and height of the largest possible\n","  axis-aligned rectangle (maximal area) within the rotated rectangle.\n","  \"\"\"\n","  if w <= 0 or h <= 0:\n","    return 0,0\n","\n","  width_is_longer = w >= h\n","  side_long, side_short = (w,h) if width_is_longer else (h,w)\n","\n","  # since the solutions for angle, -angle and 180-angle are all the same,\n","  # if suffices to look at the first quadrant and the absolute values of sin,cos:\n","  sin_a, cos_a = abs(math.sin(angle)), abs(math.cos(angle))\n","  if side_short <= 2.*sin_a*cos_a*side_long or abs(sin_a-cos_a) < 1e-10:\n","    # half constrained case: two crop corners touch the longer side,\n","    # the other two corners are on the mid-line parallel to the longer line\n","    x = 0.5 * side_short\n","    wr,hr = (x/sin_a,x/cos_a) if width_is_longer else (x/cos_a,x/sin_a)\n","  else:\n","    # fully constrained case: crop touches all 4 sides\n","    cos_2a = cos_a*cos_a - sin_a*sin_a\n","    wr,hr = (w*cos_a - h*sin_a)/cos_2a, (h*cos_a - w*sin_a)/cos_2a\n","\n","  return int(wr), int(hr)\n","\n","def gen_rotate_image(img, angle):\n","  dim = img.shape\n","  h = dim[0]\n","  w = dim[1]\n","\n","  img = rotate_image(img, angle)\n","  dim_bb = img.shape\n","  h_bb = dim_bb[0]\n","  w_bb = dim_bb[1]\n","\n","  w_r, h_r = rotated_rect_with_max_area(w, h, math.radians(angle))\n","\n","  w_0 = (w_bb-w_r) // 2\n","  h_0 = (h_bb-h_r) // 2\n","  img = img[h_0:h_0 + h_r, w_0:w_0 + w_r, :]\n","\n","  return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9UBEfqWpZ2-"},"outputs":[],"source":["# CODE FOR RAIN MASK AUGMENTATIONS\n","# code modified from https://github.com/tsingqguo/efficientderain\n","\n","def int_parameter(level, maxval):\n","  \"\"\"Helper function to scale `val` between 0 and maxval .\n","\n","  Args:\n","    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n","    maxval: Maximum value that the operation can have. This will be scaled to\n","      level/PARAMETER_MAX.\n","\n","  Returns:\n","    An int that results from scaling `maxval` according to `level`.\n","  \"\"\"\n","  return int(level * maxval / 10)\n","\n","\n","def float_parameter(level, maxval):\n","  \"\"\"Helper function to scale `val` between 0 and maxval.\n","\n","  Args:\n","    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n","    maxval: Maximum value that the operation can have. This will be scaled to\n","      level/PARAMETER_MAX.\n","\n","  Returns:\n","    A float that results from scaling `maxval` according to `level`.\n","  \"\"\"\n","  return float(level) * maxval / 10.\n","\n","\n","def sample_level(n):\n","  return np.random.uniform(low=0.1, high=n)\n","\n","\n","def autocontrast(pil_img, _):\n","  return ImageOps.autocontrast(pil_img)\n","\n","\n","def equalize(pil_img, _):\n","  return ImageOps.equalize(pil_img)\n","\n","\n","def posterize(pil_img, level):\n","  level = int_parameter(sample_level(level), 4)\n","  return ImageOps.posterize(pil_img, 4 - level)\n","\n","\n","def rotate(pil_img, level):\n","  degrees = int_parameter(sample_level(level), 30)\n","  if np.random.uniform() > 0.5:\n","    degrees = -degrees\n","  return pil_img.rotate(degrees, resample=Image.BILINEAR)\n","\n","\n","def solarize(pil_img, level):\n","  level = int_parameter(sample_level(level), 256)\n","  return ImageOps.solarize(pil_img, 256 - level)\n","\n","\n","def shear_x(pil_img, level):\n","  level = float_parameter(sample_level(level), 0.3)\n","  if np.random.uniform() > 0.5:\n","    level = -level\n","  return pil_img.transform(\n","      (pil_img.width, pil_img.height),\n","      Image.AFFINE, (1, level, 0, 0, 1, 0),\n","      resample=Image.BILINEAR)\n","\n","\n","def shear_y(pil_img, level):\n","  level = float_parameter(sample_level(level), 0.3)\n","  if np.random.uniform() > 0.5:\n","    level = -level\n","  return pil_img.transform(\n","      (pil_img.width, pil_img.height),\n","      Image.AFFINE, (1, 0, 0, level, 1, 0),\n","      resample=Image.BILINEAR)\n","\n","\n","def roll_x(pil_img, level):\n","  \n","  \"\"\"Roll an image sideways.\"\"\"\n","  delta = int_parameter(sample_level(level), pil_img.width / 3)\n","  if np.random.random() > 0.5:\n","    delta = -delta\n","  xsize, ysize = pil_img.size\n","  delta = delta % xsize\n","  if delta == 0: return pil_img\n","  part1 = pil_img.crop((0, 0, delta, ysize))\n","  part2 = pil_img.crop((delta, 0, xsize, ysize))\n","  pil_img.paste(part1, (xsize-delta, 0, xsize, ysize))\n","  pil_img.paste(part2, (0, 0, xsize-delta, ysize))\n","\n","  return pil_img\n","\n","def roll_y(pil_img, level):\n","  \"\"\"Roll an image sideways.\"\"\"\n","  delta = int_parameter(sample_level(level), pil_img.width / 3)\n","  if np.random.random() > 0.5:\n","    delta = -delta\n","  xsize, ysize = pil_img.size\n","  delta = delta % ysize\n","  if delta == 0: return pil_img\n","  part1 = pil_img.crop((0, 0, xsize, delta))\n","  part2 = pil_img.crop((0, delta, xsize, ysize))\n","  pil_img.paste(part1, (0, ysize-delta, xsize, ysize))\n","  pil_img.paste(part2, (0, 0, xsize, ysize-delta))\n","\n","  return pil_img\n","\n","# operation that overlaps with ImageNet-C's test set\n","def color(pil_img, level):\n","  level = float_parameter(sample_level(level), 1.8) + 0.1\n","  return ImageEnhance.Color(pil_img).enhance(level)\n","\n","\n","# operation that overlaps with ImageNet-C's test set\n","def contrast(pil_img, level):\n","  level = float_parameter(sample_level(level), 1.8) + 0.1\n","  return ImageEnhance.Contrast(pil_img).enhance(level)\n","\n","\n","# operation that overlaps with ImageNet-C's test set\n","def brightness(pil_img, level):\n","  level = float_parameter(sample_level(level), 1.8) + 0.1\n","  return ImageEnhance.Brightness(pil_img).enhance(level)\n","\n","\n","# operation that overlaps with ImageNet-C's test set\n","def sharpness(pil_img, level):\n","  level = float_parameter(sample_level(level), 1.8) + 0.1\n","  return ImageEnhance.Sharpness(pil_img).enhance(level)\n","\n","def zoom_x(pil_img, level):\n","  # zoom from .02 to 2.5\n","  rate = level\n","  zoom_img = pil_img.transform(\n","      (pil_img.width, pil_img.height),\n","      Image.AFFINE, (rate, 0, 0, 0, 1, 0),\n","      resample=Image.BILINEAR)\n","  # need to do reflect padding\n","  if rate > 1.0:\n","    orig_x, orig_y = pil_img.size\n","    new_x = int(orig_x/rate)\n","    zoom_img = np.array(zoom_img)\n","    zoom_img = np.pad(zoom_img[:, :new_x, :], ((0, 0), (0, orig_x-new_x), (0,0)), 'wrap')\n","  return zoom_img\n","\n","def zoom_y(pil_img, level):\n","  # zoom from .02 to 2.5\n","  rate = level\n","  zoom_img = pil_img.transform(\n","      (pil_img.width, pil_img.height),\n","      Image.AFFINE, (1, 0, 0, 0, rate, 0),\n","      resample=Image.BILINEAR)\n","  # need to do reflect padding\n","  if rate > 1.0:\n","    orig_x, orig_y = pil_img.size\n","    new_y = int(orig_y/rate)\n","    zoom_img = np.array(zoom_img)\n","    zoom_img = np.pad(zoom_img[:new_y, :, :], ((0, orig_y-new_y), (0, 0), (0,0)), 'wrap')\n","  return zoom_img\n","    \n","\n","augmentations = [\n","    rotate, shear_x, shear_y,\n","    zoom_x, zoom_y, roll_x, roll_y\n","]\n","\n","augmentations_all = [\n","    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n","    roll_x, roll_y, color, contrast, brightness, sharpness\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hs_KzHwq2bO"},"outputs":[],"source":["# RAIN MASK AUGMENTATION CODE\n","# code modified from https://github.com/tsingqguo/efficientderain\n","\n","class RandomCrop(object):\n","  def __init__(self, image_size, crop_size):\n","    self.ch, self.cw = crop_size\n","    ih, iw = image_size\n","\n","    self.h1 = random.randint(0, ih - self.ch)\n","    self.w1 = random.randint(0, iw - self.cw)\n","\n","    self.h2 = self.h1 + self.ch\n","    self.w2 = self.w1 + self.cw\n","\n","  def __call__(self, img):\n","    if len(img.shape) == 3:\n","      return img[self.h1: self.h2, self.w1: self.w2, :]\n","    else:\n","      return img[self.h1: self.h2, self.w1: self.w2]\n","\n","def getRainLayer2(rand_id1, rand_id2, rain_mask_dir):\n","  path_img_rainlayer_src = os.path.join(rain_mask_dir, f'{rand_id1}-{rand_id2}.png')\n","  rainlayer_rand = cv2.imread(path_img_rainlayer_src).astype(np.float32) / 255.0\n","  rainlayer_rand = cv2.cvtColor(rainlayer_rand, cv2.COLOR_BGR2RGB)\n","  return rainlayer_rand\n","\n","def getRandRainLayer2(rain_mask_dir):\n","  rand_id1 = random.randint(1, 165)\n","  rand_id2 = random.randint(4, 8)\n","  rainlayer_rand = getRainLayer2(rand_id1, rand_id2, rain_mask_dir)\n","  return rainlayer_rand\n","  \n","def rain_aug(img_rainy, img_gt, rain_mask_dir, zoom_min = 0.06, zoom_max = 1.8):\n","  img_rainy = (img_rainy.astype(np.float32)) / 255.0\n","  img_gt = (img_gt.astype(np.float32)) / 255.0\n","  img_rainy_ret = img_rainy\n","  img_gt_ret = img_gt\n","\n","  rainlayer_rand2 = getRandRainLayer2(rain_mask_dir)\n","  rainlayer_aug2 = augment_and_mix(rainlayer_rand2, severity = 3, width = 3, depth = -1, zoom_min = zoom_min, zoom_max = zoom_max) * 1\n","\n","  height = min(img_rainy.shape[0], rainlayer_aug2.shape[0])\n","  width = min(img_rainy.shape[1], rainlayer_aug2.shape[1])\n","  \n","  cropper = RandomCrop(rainlayer_aug2.shape[:2], (height, width))\n","  rainlayer_aug2_crop = cropper(rainlayer_aug2)\n","  cropper = RandomCrop(img_rainy.shape[:2], (height, width))\n","  img_rainy_ret = cropper(img_rainy_ret)\n","  img_gt_ret = cropper(img_gt_ret)\n","  img_rainy_ret = img_rainy_ret + rainlayer_aug2_crop - img_rainy_ret*rainlayer_aug2_crop\n","  img_rainy_ret = np.clip(img_rainy_ret, 0.0, 1.0)\n","  img_rainy_ret = (img_rainy_ret * 255).astype(np.uint8)\n","  img_gt_ret = (img_gt_ret * 255).astype(np.uint8)\n","    \n","  return img_rainy_ret, img_gt_ret\n","\n","def augment_and_mix(image, severity=3, width=3, depth=-1, alpha=1., zoom_min=0.06, zoom_max=1.8):\n","  \"\"\"Perform AugMix augmentations and compute mixture.\n","  Args:\n","    image: Raw input image as float32 np.ndarray of shape (h, w, c)\n","    severity: Severity of underlying augmentation operators (between 1 to 10).\n","    width: Width of augmentation chain\n","    depth: Depth of augmentation chain. -1 enables stochastic depth uniformly\n","      from [1, 3]\n","    alpha: Probability coefficient for Beta and Dirichlet distributions.\n","  Returns:\n","    mixed: Augmented and mixed image.\n","  \"\"\"\n","  ws = np.float32(\n","      np.random.dirichlet([alpha] * width))\n","  m = np.float32(np.random.beta(alpha, alpha))\n","\n","  mix = np.zeros_like(image)\n","  for i in range(width):\n","    image_aug = image.copy()\n","    depth = depth if depth > 0 else np.random.randint(2, 4)\n","    for _ in range(depth):\n","      op = np.random.choice(augmentations)\n","      if (op == zoom_x or op == zoom_y):\n","        rate = np.random.uniform(low=zoom_min, high=zoom_max)\n","        image_aug = apply_op(image_aug, op, rate)\n","      else:\n","        image_aug = apply_op(image_aug, op, severity)\n","    # Preprocessing commutes since all coefficients are convex\n","    mix += ws[i] * image_aug\n","    \n","  max_ws = max(ws)\n","  rate = 1.0 / max_ws  \n","  \n","  mixed = max((1 - m), 0.7) * image + max(m, rate*0.5) * mix\n","  return mixed\n","\n","def apply_op(image, op, severity):\n","  image = np.clip(image * 255., 0, 255).astype(np.uint8)\n","  pil_img = Image.fromarray(image)  # Convert to PIL.Image\n","  pil_img = op(pil_img, severity)\n","  return np.asarray(pil_img) / 255."]},{"cell_type":"markdown","metadata":{"id":"UcH9Q8l9r3gF"},"source":["# Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BejH5-WhJcoI"},"outputs":[],"source":["# DataLoaders for Training and Validation set\n","\n","class GTRainDataset(Dataset):\n","  \"\"\"\n","    The dataset class for weather net training and validation.\n","\n","    Parameters:\n","        train_dir_list (list) -- list of dirs for the dataset.\n","        val_dir_list (list) -- list of dirs for the dataset.\n","        rain_mask_dir (string) -- location of rain masks for data augmentation.\n","        img_size (int) -- size of the images after cropping.\n","        is_train (bool) -- True for training set.\n","        val_list (list) -- list of validation scenes\n","        sigma (int) -- variance for random angle rotation data augmentation\n","        zoom_min (float) -- minimum zoom for RainMix data augmentation\n","        zoom_max (float) -- maximum zoom for RainMix data augmentation\n","  \"\"\"\n","  def __init__(self, train_dir_list, val_dir_list, rain_mask_dir, img_size, is_train=True, sigma=13, zoom_min=0.06, zoom_max=1.8):\n","    super(GTRainDataset, self).__init__()\n","\n","    self.rain_mask_dir = rain_mask_dir\n","    self.img_size = img_size\n","    self.is_train = is_train\n","    self.img_paths = []\n","    self.sigma = sigma\n","    self.zoom_min = zoom_min\n","    self.zoom_max = zoom_max\n","    self.scene_indices = []\n","    last_index = 0\n","    scene_paths = []\n","    if is_train:\n","      root_dir_list = train_dir_list\n","    else:\n","      root_dir_list = val_dir_list\n","\n","    for root_dir in root_dir_list:\n","      scene_paths += [os.path.join(root_dir, scene) for scene in list(os.walk(root_dir))[0][1]]\n","    \n","    for scene_path in scene_paths:\n","      scene_img_paths = natsorted(glob(os.path.join(scene_path, '*R-*.png')))\n","      scene_length = len(scene_img_paths)\n","      self.scene_indices.append(list(range(last_index, last_index + scene_length)))\n","      last_index += scene_length\n","      self.img_paths += scene_img_paths\n","    \n","    # number of images in full dataset\n","    self.data_len = len(self.img_paths)\n","  \n","  def __len__(self):\n","    return self.data_len\n","\n","  def get_scene_indices(self):\n","    return self.scene_indices\n","  \n","  def __getitem__(self, index):\n","    ts = self.img_size\n","\n","    inp_path = self.img_paths[index]\n","    tar_path = self.img_paths[index][:-9] + 'C-000.png'\n","    if('Gurutto_1-2' in inp_path):\n","      tar_path = self.img_paths[index][:-9] + 'C' + self.img_paths[index][-10:]\n","    inp_img = Image.open(inp_path)\n","    tar_img = Image.open(tar_path)\n","\n","    # To numpy\n","    inp_img = np.array(inp_img)\n","    tar_img = np.array(tar_img)\n","\n","    if self.is_train:\n","      if random.randint(1, 10) > 4:\n","        inp_img, tar_img = rain_aug(inp_img, tar_img, self.rain_mask_dir, zoom_min=self.zoom_min, zoom_max=self.zoom_max)\n","\n","    # Random rotation\n","    if self.is_train:\n","      angle = np.random.normal(0, self.sigma)\n","      inp_img_rot = gen_rotate_image(inp_img, angle)\n","      if (inp_img_rot.shape[0] >= 256 and inp_img_rot.shape[1] >= 256):\n","        inp_img = inp_img_rot\n","        tar_img = gen_rotate_image(tar_img, angle)\n","\n","    # reflect pad and random cropping to ensure the right image size for training\n","    h,w = inp_img.shape[:2]\n","\n","    # To tensor\n","    inp_img = TF.to_tensor(inp_img)\n","    tar_img = TF.to_tensor(tar_img)\n","\n","    # reflect padding\n","    padw = ts-w if w<ts else 0\n","    padh = ts-h if h<ts else 0\n","    if padw!=0 or padh!=0:\n","      inp_img = TF.pad(inp_img, (0, 0, padw, padh), padding_mode='reflect')\n","      tar_img = TF.pad(tar_img, (0, 0, padw, padh), padding_mode='reflect')\n","    \n","    if self.is_train:\n","      # random cropping\n","      hh, ww, = inp_img.shape[1], inp_img.shape[2]\n","      rr = random.randint(0, hh-ts)\n","      cc = random.randint(0, ww-ts)\n","      inp_img = inp_img[:, rr:rr+ts, cc:cc+ts]\n","      tar_img = tar_img[:, rr:rr+ts, cc:cc+ts]\n","    else:\n","      # center cropping\n","      inp_img = TF.center_crop(inp_img, (ts, ts))\n","      tar_img = TF.center_crop(tar_img, (ts, ts))\n","\n","    # Data augmentations: flip x, flip y\n","    if self.is_train:\n","      aug = random.randint(0, 2)\n","    else:\n","      aug = 0\n","    \n","    if aug==1:\n","      inp_img = inp_img.flip(1)\n","      tar_img = tar_img.flip(1)\n","    elif aug==2:\n","      inp_img = inp_img.flip(2)\n","      tar_img = tar_img.flip(2)\n","\n","    # Get image name\n","    file_name = inp_path.split('/')[-1]\n","\n","    # Dict for return\n","    # If using tanh as the last layer, the range should be [-1, 1]\n","    sample_dict = {\n","        'input_img': inp_img * 2 - 1,\n","        'target_img': tar_img * 2 - 1,\n","        'file_name': file_name\n","    }\n","\n","    return sample_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33Bjn8-xiCtP"},"outputs":[],"source":["# Samplper for the images\n","\n","class CustomBatchSampler():\n","  def __init__(self, scene_indices, batch_size=16):\n","    self.scene_indices = scene_indices\n","    self.batch_size = batch_size\n","    self.num_batches = int(scene_indices[-1][-1]/batch_size)\n","\n","  def __len__(self):\n","    return self.num_batches\n","\n","  def __iter__(self):\n","    scene_indices = copy.deepcopy(self.scene_indices)\n","    for scene_list in scene_indices:\n","      random.shuffle(scene_list)\n","    out_indices = []\n","    done = False\n","    while not done:\n","      out_batch_indices = []\n","      if (len(scene_indices) < self.batch_size):\n","        self.num_batches = len(out_indices)\n","        return iter(out_indices)\n","      chosen_scenes = np.random.choice(len(scene_indices), self.batch_size, replace = False)\n","      empty_indices = []\n","      for i in chosen_scenes:\n","        scene_list = scene_indices[i]\n","        out_batch_indices.append(scene_list.pop())\n","        if (len(scene_list) == 0):\n","          empty_indices.append(i)\n","      empty_indices.sort(reverse=True)\n","      for i in empty_indices:\n","        scene_indices.pop(i)\n","      out_indices.append(out_batch_indices)\n","    self.num_batches = len(out_indices)\n","    return iter(out_indices)"]},{"cell_type":"markdown","metadata":{"id":"etD-mRfZr9xn"},"source":["# Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCVZQ64xXXRz"},"outputs":[],"source":["class ShiftMSSSIM(torch.nn.Module):\n","  \"\"\"Shifted SSIM Loss \"\"\"\n","  def __init__(self):\n","    super(ShiftMSSSIM, self).__init__()\n","    self.ssim = MultiScaleSSIMLoss(data_range=1.)\n","\n","  def forward(self, est, gt):\n","    # shift images back into range (0, 1)\n","    est = est * 0.5 + 0.5\n","    gt = gt *0.5 + 0.5\n","    return self.ssim(est, gt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZBAMTo5IRYD"},"outputs":[],"source":["# Rain Robust Loss\n","# Code modified from: https://github.com/sthalles/SimCLR/blob/master/simclr.py\n","\n","class RainRobustLoss(torch.nn.Module):\n","  \"\"\"Rain Robust Loss\"\"\"\n","  def __init__(self, batch_size, n_views, device, temperature=0.07):\n","    super(RainRobustLoss, self).__init__()\n","    self.batch_size = batch_size\n","    self.n_views = n_views\n","    self.temperature = temperature\n","    self.device = device\n","    self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n","\n","  def forward(self, features):\n","    logits, labels = self.info_nce_loss(features)\n","    return self.criterion(logits, labels)\n","\n","  def info_nce_loss(self, features):\n","    labels = torch.cat([torch.arange(self.batch_size) for i in range(self.n_views)], dim=0)\n","    labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n","    labels = labels.to(self.device)\n","\n","    features = F.normalize(features, dim=1)\n","\n","    similarity_matrix = torch.matmul(features, features.T)\n","\n","    # discard the main diagonal from both: labels and similarities matrix\n","    mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.device)\n","    labels = labels[~mask].view(labels.shape[0], -1)\n","    similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n","\n","    # select and combine multiple positives\n","    positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n","\n","    # select only the negatives the negatives\n","    negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n","\n","    logits = torch.cat([positives, negatives], dim=1)\n","    labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.device)\n","\n","    logits = logits / self.temperature\n","    return logits, labels"]},{"cell_type":"markdown","metadata":{"id":"lNFAQ_8nsA-7"},"source":["# Scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLmR4XJqIFsU"},"outputs":[],"source":["\"\"\"\n","Linear warmup from: \n","https://github.com/ildoonet/pytorch-gradual-warmup-lr/blob/master/warmup_scheduler/scheduler.py\n","\"\"\"\n","\n","from torch.optim.lr_scheduler import _LRScheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","class GradualWarmupScheduler(_LRScheduler):\n","  \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n","  Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n","  Args:\n","      optimizer (Optimizer): Wrapped optimizer.\n","      multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n","      total_epoch: target learning rate is reached at total_epoch, gradually\n","      after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n","  \"\"\"\n","\n","  def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","    self.multiplier = multiplier\n","    if self.multiplier < 1.:\n","      raise ValueError('multiplier should be greater thant or equal to 1.')\n","    self.total_epoch = total_epoch\n","    self.after_scheduler = after_scheduler\n","    self.finished = False\n","    super(GradualWarmupScheduler, self).__init__(optimizer)\n","\n","  def get_lr(self):\n","    if self.last_epoch > self.total_epoch:\n","      if self.after_scheduler:\n","        if not self.finished:\n","          self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n","          self.finished = True\n","        return self.after_scheduler.get_last_lr()\n","      return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","\n","    if self.multiplier == 1.0:\n","      return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","    else:\n","      return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","\n","  def step_ReduceLROnPlateau(self, metrics, epoch=None):\n","    if epoch is None:\n","      epoch = self.last_epoch + 1\n","    self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n","    if self.last_epoch <= self.total_epoch:\n","      warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","      for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n","        param_group['lr'] = lr\n","    else:\n","      if epoch is None:\n","        self.after_scheduler.step(metrics, None)\n","      else:\n","        self.after_scheduler.step(metrics, epoch - self.total_epoch)\n","\n","  def step(self, epoch=None, metrics=None):\n","    if type(self.after_scheduler) != ReduceLROnPlateau:\n","      if self.finished and self.after_scheduler:\n","        if epoch is None:\n","          self.after_scheduler.step(None)\n","        else:\n","          self.after_scheduler.step(epoch - self.total_epoch)\n","        self._last_lr = self.after_scheduler.get_last_lr()\n","      else:\n","        return super(GradualWarmupScheduler, self).step(epoch)\n","    else:\n","      self.step_ReduceLROnPlateau(metrics, epoch)"]},{"cell_type":"markdown","metadata":{"id":"aRS3BSIFsD9p"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6yE2Yyvn_n3"},"outputs":[],"source":["# Main network blocks\n","# Code modified from: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n","\n","# Basic Blocks\n","class Identity(nn.Module):\n","  def forward(self, x):\n","    return x\n","\n","def get_norm_layer(norm_type='instance'):\n","  \"\"\"Return a normalization layer\n","  Parameters:\n","      norm_type (str) -- the name of the normalization layer: batch | instance | none\n","  For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n","  For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n","  \"\"\"\n","  if norm_type == 'batch':\n","    norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n","  elif norm_type == 'instance':\n","    norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n","  elif norm_type == 'none':\n","    def norm_layer(x): return Identity()\n","  else:\n","    raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n","  return norm_layer\n","\n","class Conv2d(torch.nn.Module):\n","  '''\n","  2D convolution class\n","  Args:\n","    in_channels : int\n","      number of input channels\n","    out_channels : int\n","      number of output channels\n","    kernel_size : int\n","      size of kernel\n","    stride : int\n","      stride of convolution\n","    activation_func : func\n","      activation function after convolution\n","    norm_layer : functools.partial\n","      normalization layer\n","    use_bias : bool\n","      if set, then use bias\n","    padding_type : str\n","      the name of padding layer: reflect | replicate | zero\n","  '''\n","\n","  def __init__(\n","      self,\n","      in_channels,\n","      out_channels,\n","      kernel_size=3,\n","      stride=1,\n","      activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n","      norm_layer=nn.BatchNorm2d,\n","      use_bias=False,\n","      padding_type='reflect'):\n","    \n","    super(Conv2d, self).__init__()\n","    \n","    self.activation_func = activation_func\n","    conv_block = []\n","    p = 0\n","    if padding_type == 'reflect':\n","      conv_block += [nn.ReflectionPad2d(kernel_size // 2)]\n","    elif padding_type == 'replicate':\n","      conv_block += [nn.ReplicationPad2d(kernel_size // 2)]\n","    elif padding_type == 'zero':\n","      p = kernel_size // 2\n","    else:\n","      raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n","\n","    conv_block += [\n","      nn.Conv2d(\n","          in_channels, \n","          out_channels, \n","          stride=stride,\n","          kernel_size=kernel_size, \n","          padding=p, \n","          bias=use_bias), \n","      norm_layer(out_channels)]\n","\n","    self.conv = nn.Sequential(*conv_block)\n","\n","  def forward(self, x):\n","    conv = self.conv(x)\n","\n","    if self.activation_func is not None:\n","      return self.activation_func(conv)\n","    else:\n","      return conv\n","\n","class DeformableConv2d(nn.Module):\n","  '''\n","  2D deformable convolution class\n","  Args:\n","    in_channels : int\n","      number of input channels\n","    out_channels : int\n","      number of output channels\n","    kernel_size : int\n","      size of kernel\n","    stride : int\n","      stride of convolution\n","    padding : int\n","      padding\n","    use_bias : bool\n","      if set, then use bias\n","  '''\n","  def __init__(\n","      self,\n","      in_channels,\n","      out_channels,\n","      kernel_size=3,\n","      stride=1,\n","      padding=1,\n","      bias=False):\n","    \n","    super(DeformableConv2d, self).__init__()\n","      \n","    self.stride = stride if type(stride) == tuple else (stride, stride)\n","    self.padding = padding\n","    \n","    self.offset_conv = nn.Conv2d(\n","        in_channels, \n","        2 * kernel_size * kernel_size,\n","        kernel_size=kernel_size, \n","        stride=stride,\n","        padding=self.padding, \n","        bias=True)\n","\n","    nn.init.constant_(self.offset_conv.weight, 0.)\n","    nn.init.constant_(self.offset_conv.bias, 0.)\n","    \n","    self.modulator_conv = nn.Conv2d(\n","        in_channels, \n","        1 * kernel_size * kernel_size,\n","        kernel_size=kernel_size, \n","        stride=stride,\n","        padding=self.padding, \n","        bias=True)\n","\n","    nn.init.constant_(self.modulator_conv.weight, 0.)\n","    nn.init.constant_(self.modulator_conv.bias, 0.)\n","    \n","    self.regular_conv = nn.Conv2d(\n","        in_channels=in_channels,\n","        out_channels=out_channels,\n","        kernel_size=kernel_size,\n","        stride=stride,\n","        padding=self.padding,\n","        bias=bias)\n","\n","  def forward(self, x):\n","    offset = self.offset_conv(x)\n","    modulator = 2. * torch.sigmoid(self.modulator_conv(x))\n","    \n","    x = torchvision.ops.deform_conv2d(\n","        input=x, \n","        offset=offset, \n","        weight=self.regular_conv.weight, \n","        bias=self.regular_conv.bias, \n","        padding=self.padding,\n","        mask=modulator,\n","        stride=self.stride)\n","    return x\n","\n","\n","class UpConv2d(torch.nn.Module):\n","  '''\n","  Up-convolution (upsample + convolution) block class\n","  Args:\n","    in_channels : int\n","      number of input channels\n","    out_channels : int\n","      number of output channels\n","    kernel_size : int\n","      size of kernel (k x k)\n","    activation_func : func\n","      activation function after convolution\n","    norm_layer : functools.partial\n","      normalization layer\n","    use_bias : bool\n","      if set, then use bias\n","    padding_type : str\n","      the name of padding layer: reflect | replicate | zero\n","    interpolate_mode : str\n","      the mode for interpolation: bilinear | nearest\n","  '''\n","  def __init__(\n","      self,\n","      in_channels,\n","      out_channels,\n","      kernel_size=3,\n","      activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n","      norm_layer=nn.BatchNorm2d,\n","      use_bias=False,\n","      padding_type='reflect',\n","      interpolate_mode='bilinear'):\n","    \n","    super(UpConv2d, self).__init__()\n","    self.interpolate_mode = interpolate_mode\n","\n","    self.conv = Conv2d(\n","      in_channels,\n","      out_channels,\n","      kernel_size=kernel_size,\n","      stride=1,\n","      activation_func=activation_func,\n","      norm_layer=norm_layer,\n","      use_bias=use_bias,\n","      padding_type=padding_type)\n","\n","  def forward(self, x):\n","    n_height, n_width = x.shape[2:4]\n","    shape = (int(2 * n_height), int(2 * n_width))\n","    upsample = torch.nn.functional.interpolate(\n","      x, size=shape, mode=self.interpolate_mode, align_corners=True)\n","    conv = self.conv(upsample)\n","    return conv\n","\n","class DeformableResnetBlock(nn.Module):\n","  \"\"\"Define a Resnet block with deformable convolutions\"\"\"\n","  \n","  def __init__(\n","    self, dim, padding_type, \n","    norm_layer, use_dropout, \n","    use_bias, activation_func):\n","  \n","    \"\"\"Initialize the deformable Resnet block\n","    A defromable resnet block is a conv block with skip connections\n","    \"\"\"\n","    super(DeformableResnetBlock, self).__init__()\n","    self.conv_block = self.build_conv_block(\n","        dim, padding_type, \n","        norm_layer, use_dropout, \n","        use_bias, activation_func)\n","\n","  def build_conv_block(\n","    self, dim, padding_type, \n","    norm_layer, use_dropout, \n","    use_bias, activation_func):\n","    \"\"\"Construct a convolutional block.\n","    Parameters:\n","        dim (int) -- the number of channels in the conv layer.\n","        padding_type (str) -- the name of padding layer: reflect | replicate | zero\n","        norm_layer -- normalization layer\n","        use_dropout (bool) -- if use dropout layers.\n","        use_bias (bool) -- if the conv layer uses bias or not\n","        activation_func (func) -- activation type\n","    Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer)\n","    \"\"\"\n","    conv_block = []\n","\n","    p = 0\n","    if padding_type == 'reflect':\n","      conv_block += [nn.ReflectionPad2d(1)]\n","    elif padding_type == 'replicate':\n","      conv_block += [nn.ReplicationPad2d(1)]\n","    elif padding_type == 'zero':\n","      p = 1\n","    else:\n","      raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n","\n","    conv_block += [\n","        DeformableConv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), \n","        norm_layer(dim), \n","        activation_func]\n","    \n","    if use_dropout:\n","        conv_block += [nn.Dropout(0.5)]\n","\n","    p = 0\n","    if padding_type == 'reflect':\n","      conv_block += [nn.ReflectionPad2d(1)]\n","    elif padding_type == 'replicate':\n","      conv_block += [nn.ReplicationPad2d(1)]\n","    elif padding_type == 'zero':\n","      p = 1\n","    else:\n","      raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n","    conv_block += [DeformableConv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n","\n","    return nn.Sequential(*conv_block)\n","\n","  def forward(self, x):\n","    \"\"\"Forward function (with skip connections)\"\"\"\n","    out = x + self.conv_block(x)    # add skip connections\n","    return out\n","\n","class DecoderBlock(torch.nn.Module):\n","  '''\n","  Decoder block with skip connections\n","  Args:\n","    in_channels : int\n","      number of input channels\n","    skip_channels : int\n","      number of skip connection channels\n","    out_channels : int\n","      number of output channels\n","    activation_func : func\n","      activation function after convolution\n","    norm_layer : functools.partial\n","      normalization layer\n","    use_bias : bool\n","      if set, then use bias\n","    padding_type : str\n","      the name of padding layer: reflect | replicate | zero\n","    upsample_mode : str\n","      the mode for interpolation: transpose | bilinear | nearest\n","  '''\n","\n","  def __init__(\n","      self,\n","      in_channels,\n","      skip_channels,\n","      out_channels,\n","      activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n","      norm_layer=nn.BatchNorm2d,\n","      use_bias=False,\n","      padding_type='reflect',\n","      upsample_mode='transpose'):\n","    super(DecoderBlock, self).__init__()\n","\n","    self.skip_channels = skip_channels\n","    self.upsample_mode = upsample_mode\n","    \n","    # Upsampling\n","    if upsample_mode == 'transpose':\n","      self.deconv = nn.Sequential(\n","          nn.ConvTranspose2d(\n","              in_channels, out_channels,\n","              kernel_size=3, stride=2,\n","              padding=1, output_padding=1,\n","              bias=use_bias),\n","          norm_layer(out_channels),\n","          activation_func)\n","    else:\n","      self.deconv = UpConv2d(\n","          in_channels, out_channels,\n","          use_bias=use_bias,\n","          activation_func=activation_func,\n","          norm_layer=norm_layer,\n","          padding_type=padding_type,\n","          interpolate_mode=upsample_mode)\n","\n","    concat_channels = skip_channels + out_channels\n","    \n","    self.conv = Conv2d(\n","        concat_channels,\n","        out_channels,\n","        kernel_size=3,\n","        stride=1,\n","        activation_func=activation_func,\n","        padding_type=padding_type,\n","        norm_layer=norm_layer,\n","        use_bias=use_bias)\n","\n","  def forward(self, x, skip=None):\n","    deconv = self.deconv(x)\n","\n","    if self.skip_channels > 0:\n","      concat = torch.cat([deconv, skip], dim=1)\n","    else:\n","      concat = deconv\n","\n","    return self.conv(concat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHXc932d20JB"},"outputs":[],"source":["def init_weights(net, init_type='normal', init_gain=0.02):\n","  \"\"\"\n","  Initialize network weights.\n","  Parameters:\n","      net (network) -- network to be initialized\n","      init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n","      init_gain (float) -- scaling factor for normal, xavier and orthogonal.\n","  \"\"\"\n","  def init_func(m):  # define the initialization function\n","    classname = m.__class__.__name__\n","    if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n","      if init_type == 'normal':\n","        init.normal_(m.weight.data, 0.0, init_gain)\n","      elif init_type == 'xavier':\n","        init.xavier_normal_(m.weight.data, gain=init_gain)\n","      elif init_type == 'kaiming':\n","        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n","      elif init_type == 'orthogonal':\n","        init.orthogonal_(m.weight.data, gain=init_gain)\n","      else:\n","        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n","      if hasattr(m, 'bias') and m.bias is not None:\n","        init.constant_(m.bias.data, 0.0)\n","    elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n","      init.normal_(m.weight.data, 1.0, init_gain)\n","      init.constant_(m.bias.data, 0.0)\n","\n","  print('initialize network with %s' % init_type)\n","  net.apply(init_func)  # apply the initialization function <init_func>\n","\n","def init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[]):\n","  \"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n","  Parameters:\n","          net (network) -- the network to be initialized\n","          init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n","          gain (float) -- scaling factor for normal, xavier and orthogonal.\n","          gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n","  Return an initialized network.\n","  \"\"\"\n","  if len(gpu_ids) > 0:\n","    assert(torch.cuda.is_available())\n","    net.to(gpu_ids[0])\n","    net = torch.nn.DataParallel(net, gpu_ids)    # multi-GPUs\n","  init_weights(net, init_type, init_gain=init_gain)\n","  \n","  # Zero for deform convs\n","  key_name_list = ['offset', 'modulator']\n","  for cur_name, parameters in net.named_parameters():\n","    if any(key_name in cur_name for key_name in key_name_list):\n","      nn.init.constant_(parameters, 0.)\n","  return net\n","\n","class ResNetModified(nn.Module):\n","  \"\"\"\n","  Resnet-based generator that consists of deformable Resnet blocks.\n","  \"\"\"\n","\n","  def __init__(\n","      self, \n","      input_nc, \n","      output_nc, \n","      ngf=64, \n","      norm_layer=nn.BatchNorm2d, \n","      activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n","      use_dropout=False, \n","      n_blocks=6, \n","      padding_type='reflect',\n","      upsample_mode='bilinear'):\n","    \"\"\"Construct a Resnet-based generator\n","    Parameters:\n","      input_nc (int) -- the number of channels in input images\n","      output_nc (int) -- the number of channels in output images\n","      ngf (int) -- the number of filters in the last conv layer\n","      norm_layer -- normalization layer\n","      use_dropout (bool) -- if use dropout layers\n","      n_blocks (int) -- the number of ResNet blocks\n","      padding_type (str) -- the name of padding layer in conv layers: reflect | replicate | zero\n","      upsample_mode (str) -- mode for upsampling: transpose | bilinear\n","    \"\"\"\n","    assert(n_blocks >= 0)\n","    super(ResNetModified, self).__init__()\n","    if type(norm_layer) == functools.partial:\n","      use_bias = norm_layer.func == nn.InstanceNorm2d\n","    else:\n","      use_bias = norm_layer == nn.InstanceNorm2d\n","\n","    # Initial Convolution\n","    self.initial_conv = nn.Sequential(\n","        Conv2d(\n","            in_channels=input_nc,\n","            out_channels=ngf,\n","            kernel_size=7,\n","            padding_type=padding_type,\n","            norm_layer=norm_layer,\n","            activation_func=activation_func,\n","            use_bias=use_bias),\n","        Conv2d(\n","            in_channels=ngf,\n","            out_channels=ngf,\n","            kernel_size=3,\n","            padding_type=padding_type,\n","            norm_layer=norm_layer,\n","            activation_func=activation_func,\n","            use_bias=use_bias))\n","\n","    # Downsample Blocks\n","    n_downsampling = 2\n","    mult = 2 ** 0\n","    self.downsample_1 = Conv2d(\n","        in_channels=ngf * mult,\n","        out_channels=ngf * mult * 2,\n","        kernel_size=3,\n","        stride=2,\n","        padding_type=padding_type,\n","        norm_layer=norm_layer,\n","        activation_func=activation_func,\n","        use_bias=use_bias)\n","    \n","    mult = 2 ** 1\n","    self.downsample_2 = Conv2d(\n","        in_channels=ngf * mult,\n","        out_channels=ngf * mult * 2,\n","        kernel_size=3,\n","        stride=2,\n","        padding_type=padding_type,\n","        norm_layer=norm_layer,\n","        activation_func=activation_func,\n","        use_bias=use_bias)\n","\n","    # Residual Blocks\n","    residual_blocks = []\n","    mult = 2 ** n_downsampling\n","    for i in range(n_blocks): # add ResNet blocks\n","      residual_blocks += [\n","          DeformableResnetBlock(\n","              ngf * mult, \n","              padding_type=padding_type, \n","              norm_layer=norm_layer, \n","              use_dropout=use_dropout, \n","              use_bias=use_bias, activation_func=activation_func)]\n","\n","    self.residual_blocks = nn.Sequential(*residual_blocks)\n","\n","    # Upsampling\n","    mult = 2 ** (n_downsampling - 0)\n","    self.upsample_2 = DecoderBlock(\n","        ngf * mult, \n","        int(ngf * mult / 2),\n","        int(ngf * mult / 2),\n","        use_bias=use_bias,\n","        activation_func=activation_func,\n","        norm_layer=norm_layer,\n","        padding_type=padding_type,\n","        upsample_mode=upsample_mode)\n","    \n","    mult = 2 ** (n_downsampling - 1)\n","    self.upsample_1 = DecoderBlock(\n","        ngf * mult, \n","        int(ngf * mult / 2),\n","        int(ngf * mult / 2),\n","        use_bias=use_bias,\n","        activation_func=activation_func,\n","        norm_layer=norm_layer,\n","        padding_type=padding_type,\n","        upsample_mode=upsample_mode)\n","    \n","    # Output Convolution\n","    self.output_conv_naive = nn.Sequential(\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(ngf, output_nc, kernel_size=3, padding=0),\n","        nn.Tanh())\n","\n","    # Projection for rain robust loss\n","    self.feature_projection = nn.Sequential(\n","        nn.AdaptiveAvgPool2d((2, 2)),\n","        nn.Flatten(start_dim=1, end_dim=-1))\n","\n","  def forward(self, input):\n","    \"\"\"Standard forward\"\"\"\n","\n","    # Downsample\n","    initial_conv_out  = self.initial_conv(input)\n","    downsample_1_out = self.downsample_1(initial_conv_out)\n","    downsample_2_out = self.downsample_2(downsample_1_out)\n","\n","    # Residual\n","    residual_blocks_out = self.residual_blocks(downsample_2_out)\n","\n","    # Upsample\n","    upsample_2_out = self.upsample_2(residual_blocks_out, downsample_1_out)\n","    upsample_1_out = self.upsample_1(upsample_2_out, initial_conv_out)\n","    final_out = self.output_conv_naive(upsample_1_out)\n","\n","    # Features\n","    features = self.feature_projection(residual_blocks_out)\n","\n","    # Return multiple final conv results\n","    return final_out, features\n","\n","class GTRainModel(nn.Module):\n","  def __init__(\n","      self, \n","      ngf=64,\n","      n_blocks=9,\n","      norm_layer_type='batch',\n","      activation_func=torch.nn.LeakyReLU(negative_slope=0.10, inplace=True),\n","      upsample_mode='bilinear',\n","      init_type='kaiming'):\n","    \"\"\"\n","    GT-Rain Model\n","    Parameters:\n","      ngf (int) -- the number of conv filters\n","      n_blocks (int) -- the number of deformable ResNet blocks\n","      norm_layer_type (str) -- 'batch', 'instance'\n","      activation_func (func) -- activation functions\n","      upsample_mode (str) -- 'transpose', 'bilinear'\n","      init_type (str) -- None, 'normal', 'xavier', 'kaiming', 'orthogonal'\n","    \"\"\"\n","    super(GTRainModel, self).__init__()\n","    self.resnet = ResNetModified(\n","        input_nc=3, output_nc=3, ngf=ngf, \n","        norm_layer=get_norm_layer(norm_layer_type),\n","        activation_func=activation_func,\n","        use_dropout=False, n_blocks=n_blocks, \n","        padding_type='reflect',\n","        upsample_mode=upsample_mode)\n","\n","    # Initialization\n","    if init_type:\n","      init_net(self.resnet, init_type=init_type)\n","\n","  def forward(self, x, clear_img):\n","    input_cat = torch.cat((x, clear_img), dim=0)\n","    out_img, out_feature = self.resnet(input_cat)\n","    return out_img[:x.shape[0], ...], out_feature "]},{"cell_type":"markdown","metadata":{"id":"TQ754ooeKxkV"},"source":["# Training Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3p83EuSI376a"},"outputs":[],"source":["# Parameters\n","params = {\n","  'batch_size': 8, # batch size\n","  'num_epochs': 3, # number of epochs to train\n","  'warmup_epochs': 4, # number of epochs for warmup\n","  'initial_lr': 2e-4, # initial learning rate used by scheduler\n","  'min_lr': 1e-6, # minimum learning rate used by scheduler\n","  'val_epoch': 1, # validation done every k epochs\n","  'init_type': 'normal', #'xavier', # Initialization type \n","  'ngf': 64, # the number of channels for the model capacity\n","  'n_blocks': 9, # the number of blocks in ResNet\n","  'norm_layer_type': 'batch', # Normalization type\n","  'activation_func': torch.nn.LeakyReLU(negative_slope=0.10, inplace=True), # Activation function\n","  'upsample_mode': 'bilinear', # Mode for upsampling\n","  'save_dir': './checkpoints/weights_dir', # Dir to save the model weights\n","  'save_every': 1, # save every k epoch\n","  'train_dir_list': [\n","      './train_dir', \n","    ], # Dir for the training data\n","  'val_dir_list': ['./val_dir'], # Dir for the val data\n","  'rain_mask_dir': './rain_mask_dir', # Dir for the rain masks\n","  'img_size': 256, # the size of image input\n","  'zoom_min': .06, # the minimum zoom for RainMix\n","  'zoom_max': 1.8, # the maximum zoom for RainMix\n","  'l1_loss_weight': 0.1, # weight for l1 loss\n","  'ssim_loss_weight': 1.0, # weight for the ssim loss\n","  'robust_loss_weight': 0.1, # weight for rain robust loss\n","  'temperature': 0.25, # Temperature for the rain robust loss\n","  'resume_train': False, # begin training using loaded checkpoint\n","  'model_path': None, # Dir to load model weights\n","  'tensorboard_log_step_train': 100, # Number of steps to log into tensorboard when training\n","  'tensorboard_log_step_val': 1, # This number will be updated automatically based after creating the dataloaders\n","}\n","\n","# Create dir to save the weights\n","Path(params['save_dir']).mkdir(parents=True, exist_ok=True)\n","\n","# Set up tensorboard SummaryWriter and directories\n","writer = SummaryWriter(os.path.join(params['save_dir'], 'tensorboard'))"]},{"cell_type":"markdown","metadata":{"id":"mB2MQZhnsJ5F"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hxtfpu6kaV9"},"outputs":[],"source":["# Create the DataLoaders for training and validation\n","train_dataset = GTRainDataset(\n","    train_dir_list=params['train_dir_list'],\n","    val_dir_list=params['val_dir_list'], \n","    rain_mask_dir=params['rain_mask_dir'],\n","    img_size=params['img_size'], \n","    is_train=True, \n","    zoom_min = params['zoom_min'],\n","    zoom_max = params['zoom_max'])\n","\n","train_loader = DataLoader(\n","    dataset=train_dataset, \n","    batch_sampler=CustomBatchSampler(\n","        train_dataset.get_scene_indices(), \n","        batch_size=params['batch_size']),\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","val_dataset = GTRainDataset(\n","    train_dir_list=params['train_dir_list'],\n","    val_dir_list=params['val_dir_list'], \n","    rain_mask_dir=params['rain_mask_dir'],\n","    img_size=params['img_size'],\n","    is_train=False)\n","\n","val_loader = DataLoader(\n","    dataset=val_dataset, \n","    batch_size=params['batch_size'], \n","    num_workers=2, \n","    shuffle=True, \n","    drop_last=True, \n","    pin_memory=True)\n","\n","print('Train set length:', len(train_dataset))\n","print('Val set lenth:', len(val_dataset))\n","\n","# Adjust the log freq based on the number of training and val samples\n","params['tensorboard_log_step_val'] = int(params['tensorboard_log_step_train'] * len(val_dataset) / len(train_dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wdeVAxXlN_sY"},"outputs":[],"source":["\"\"\"\n","Script for training\n","\"\"\"\n","\n","# Make the model\n","model = GTRainModel(\n","    ngf=params['ngf'],\n","    n_blocks=params['n_blocks'],\n","    norm_layer_type=params['norm_layer_type'],\n","    activation_func=params['activation_func'],\n","    upsample_mode=params['upsample_mode'],\n","    init_type=params['init_type'])\n","\n","print(model)\n","model.cuda()\n","\n","# Setting up the optimizer and LR scheduler\n","# Different learning rate for the deformable groups\n","key_name_list = ['offset', 'modulator']\n","deform_params = []\n","normal_params = []\n","for cur_name, parameters in model.named_parameters():\n","  if any(key_name in cur_name for key_name in key_name_list):\n","    deform_params.append(parameters)\n","  else:\n","    normal_params.append(parameters)\n","print('deform:', len(deform_params), 'normal:', len(normal_params))\n","\n","optimizer = optim.Adam(\n","    [{\"params\": normal_params},\n","     {\"params\": deform_params, \"lr\": params['initial_lr']/10}], \n","    lr=params['initial_lr'], \n","    betas=(0.9, 0.999), \n","    eps=1e-8)\n","\n","scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(\n","    optimizer, \n","    params['num_epochs'] - params['warmup_epochs'], \n","    eta_min=params['min_lr'])\n","\n","scheduler = GradualWarmupScheduler(\n","    optimizer, \n","    multiplier=1.0, \n","    total_epoch=params['warmup_epochs'], \n","    after_scheduler=scheduler_cosine)\n","\n","optimizer.zero_grad()\n","optimizer.step()\n","scheduler.step() # To start warmup\n","\n","# Setting up Loss Function\n","criterion_l1 = nn.L1Loss().cuda()\n","criterion_neg_ssim = ShiftMSSSIM().cuda() \n","criterion_robust = RainRobustLoss(\n","    batch_size=params['batch_size'], \n","    n_views=2, \n","    device=torch.device(\"cuda\"),\n","    temperature=params['temperature']).cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBgwOl6BtrG9"},"outputs":[],"source":["start_epoch = 0\n","\n","if params['resume_train']:\n","  print(f\"Loading checkpoint {params['model_path']}\")\n","  checkpoint = torch.load(params['model_path'])\n","\n","  # Load Model\n","  model.load_state_dict(checkpoint['state_dict'])\n","  start_epoch = checkpoint['epoch'] + 1\n","  print(f\"Resuming epoch: {start_epoch}\")\n","  \n","  for i in range(start_epoch):\n","    scheduler.step()\n","\n","  print(f\"Resuming lr: {optimizer.param_groups[0]['lr']}\")\n","else:\n","  print(f\"Initial lr: {optimizer.param_groups[0]['lr']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaHVEnOxwPe4"},"outputs":[],"source":["# TRAINING AND VALIDATION\n","best_epoch = 0\n","best_psnr = 0\n","for epoch in range(start_epoch, params['num_epochs']):\n","  epoch_start_time = time.time()\n","  epoch_loss = 0\n","\n","  # TRAINING\n","  model.train()\n","  train_loop = tqdm(train_loader, leave=False, position=0)\n","  train_loop.set_description(f\"Epoch {epoch}/{params['num_epochs']}\")\n","  for batch_idx, batch_data in enumerate(train_loop):\n","        \n","    # Load the data\n","    input_img = batch_data['input_img'].cuda()\n","    target_img = batch_data['target_img'].cuda()\n","\n","    # Check the current step\n","    current_step = epoch * len(train_loader) + batch_idx\n","\n","    #########\n","    # Train #\n","    #########\n","\n","    # Zero the gradients\n","    optimizer.zero_grad()\n","    \n","    # Forward pass through the entire model (both parts)\n","    output_img, output_features = model(input_img, target_img)\n","\n","    # Calculate losses between gt and output\n","    loss = 0\n","\n","    if params['l1_loss_weight']:\n","      loss_l1 = criterion_l1(output_img, target_img)\n","      loss += params['l1_loss_weight'] * loss_l1\n","      loss_l1_log = loss_l1.item()\n","\n","    if params['ssim_loss_weight']:\n","      loss_ssim = criterion_neg_ssim(output_img, target_img)\n","      loss += params['ssim_loss_weight'] * loss_ssim\n","      loss_ssim_log = loss_ssim.item()\n","\n","    if params['robust_loss_weight']:\n","      loss_robust = criterion_robust(output_features)\n","      loss += params['robust_loss_weight'] * loss_robust\n","      loss_robust_log = loss_robust.item()\n","\n","    # Backwards pass and step\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Log\n","    epoch_loss += loss.item()\n","    loss_log = loss.item()\n","\n","    # Tensorboard\n","    if (current_step % params['tensorboard_log_step_train']) == 0:\n","      \n","      # Log loss\n","      writer.add_scalar('loss/train', loss_log, current_step)\n","\n","      # Seperate loss\n","      if params['l1_loss_weight']:\n","        writer.add_scalar('l1_loss/train', loss_l1_log, current_step)\n","      if params['ssim_loss_weight']:\n","        writer.add_scalar('ssim_loss/train', loss_ssim_log, current_step)\n","      if params['robust_loss_weight']:\n","        writer.add_scalar('robust_loss/train', loss_robust_log, current_step)\n","\n","      # Log images \n","      log_img = torchvision.utils.make_grid(\n","        torch.cat([\n","            input_img[0:2, ...].cpu() * 0.5 + 0.5,\n","            output_img[0:2, ...].detach().cpu() * 0.5 + 0.5,\n","            target_img[0:2, ...].cpu() * 0.5 + 0.5], dim=-1), nrow=1)\n","      \n","      log_img_difference = torchvision.utils.make_grid(\n","          torch.cat([\n","          torch.abs(input_img[0:2, ...].cpu() - target_img[0:2, ...].cpu()) / 2,\n","          torch.abs(input_img[0:2, ...].cpu() - output_img[0:2, ...].detach().cpu()) / 2,\n","          torch.abs(output_img[0:2, ...].detach().cpu() - target_img[0:2, ...].cpu()) / 2,\n","          ], dim=-1), nrow=1)\n","      writer.add_image(\n","        'images/train (input, out, gt)', \n","        log_img, current_step)\n","      writer.add_image(\n","        'images_diff/train (input-gt, input-out, out-gt)', \n","        log_img_difference, current_step)\n","  \n","  # Print info\n","  print(\n","  f\"Epoch: {epoch}\\t\"\n","  f\"Time: {time.time() - epoch_start_time:.4f}\\n\"\n","  f\"Train Loss: {epoch_loss / len(train_loader):.4f}\\t\"\n","  f\"Learning Rate First {optimizer.param_groups[0]['lr']:.8f}\\t\")\n","\n","  # Log images \n","  log_img = torchvision.utils.make_grid(\n","    torch.cat([\n","        input_img[0:2, ...].cpu() * 0.5 + 0.5,\n","        output_img[0:2, ...].detach().cpu() * 0.5 + 0.5,\n","        target_img[0:2, ...].cpu() * 0.5 + 0.5], dim=-1), nrow=1)\n","  \n","  log_img = log_img.permute(1, 2, 0)\n","\n","  log_img_difference = torchvision.utils.make_grid(\n","    torch.cat([\n","        torch.abs(input_img[0:2, ...].cpu() - target_img[0:2, ...].cpu()) / 2,\n","        torch.abs(input_img[0:2, ...].cpu() - output_img[0:2, ...].detach().cpu()) / 2,\n","        torch.abs(output_img[0:2, ...].detach().cpu() - target_img[0:2, ...].cpu()) / 2\n","        ], dim=-1), nrow=1)\n","  \n","  log_img_difference = log_img_difference.permute(1, 2, 0)\n","  \n","  # Show image\n","  show_img(\n","      img=log_img,\n","      title=f'Train Result (input, out, gt) in epoch {epoch}')\n","  show_img(\n","      img=log_img_difference,\n","      title=f'Train Result (input-gt, input-out, out-gt) in epoch {epoch}',\n","      figsize=(9, 9))\n","\n","  ##############\n","  # Validation #\n","  ##############\n","\n","  if epoch %  params['val_epoch'] == 0:\n","    model.eval()\n","    epoch_start_time = time.time()\n","    epoch_loss = 0\n","\n","    val_loop = tqdm(val_loader, leave=False, position=0)\n","    val_loop.set_description('Val Epoch')\n","    for batch_idx, batch_data in enumerate(val_loop):\n","      \n","      # Load data\n","      input_img = batch_data['input_img'].cuda()\n","      target_img = batch_data['target_img'].cuda()\n","\n","      # Check the current step\n","      current_step = epoch * len(val_loader) + batch_idx\n","\n","      # Forward pass of model\n","      with torch.no_grad():\n","\n","        output_img, output_feature = model(input_img, target_img)\n","        \n","        # Calculate losses between pseudo-gt and output\n","        loss = 0\n","        if params['l1_loss_weight']:\n","          loss_l1 = criterion_l1(output_img, target_img)\n","          loss += params['l1_loss_weight'] * loss_l1\n","          loss_l1_log = loss_l1.item()\n","\n","        if params['ssim_loss_weight']:\n","          loss_ssim = criterion_neg_ssim(output_img, target_img)\n","          loss += params['ssim_loss_weight'] * loss_ssim\n","          loss_ssim_log = loss_ssim.item()\n","        \n","        epoch_loss += loss.item()\n","        loss_log = loss.item()\n","      \n","        # Tensorboard\n","        if (current_step % params['tensorboard_log_step_val']) == 0:\n","          \n","          # Log loss\n","          writer.add_scalar('loss/val', loss_log, current_step)\n","\n","          # Seperate loss\n","          if params['l1_loss_weight']:\n","            writer.add_scalar('l1_loss/val', loss_l1_log, current_step)\n","          if params['ssim_loss_weight']:\n","            writer.add_scalar('ssim_loss/val', loss_ssim_log, current_step)\n","\n","          # Log images \n","          log_img = torchvision.utils.make_grid(\n","              torch.cat([\n","                  input_img[0:2, ...].cpu() * 0.5 + 0.5,\n","                  output_img[0:2, ...].cpu() * 0.5 + 0.5,\n","                  target_img[0:2, ...].cpu() * 0.5 + 0.5], dim=-1), nrow=1)\n","          writer.add_image('images/val (input-out-gt)', log_img, current_step)\n","      \n","    # Print info\n","    avg_val_loss = epoch_loss / len(val_loader)\n","    print(\n","        f\"Val Epoch\\t\"\n","        f\"Time: {time.time() - epoch_start_time:.4f}\\t\"\n","        f\"Val Loss: {avg_val_loss:.4f}\")\n","    \n","    # Log images \n","    log_img = torchvision.utils.make_grid(\n","        torch.cat([\n","            input_img[0:2, ...].cpu() * 0.5 + 0.5,\n","            output_img[0:2, ...].cpu() * 0.5 + 0.5,\n","            target_img[0:2, ...].cpu() * 0.5 + 0.5], dim=-1), nrow=1)\n","    log_img = log_img.permute(1, 2, 0)\n","    \n","    # Show image\n","    show_img(\n","        img=log_img,\n","        title=f'Val Result (input, out, gt) in epoch {epoch}')\n","  \n","  # Move the scheduler forward\n","  scheduler.step()\n","\n","  # Save every few epochs\n","  if epoch % params['save_every'] == 0:\n","    print('Saving...')\n","    torch.save({\n","        'epoch': epoch, \n","        'state_dict': model.state_dict(),\n","        'optimizer' : optimizer.state_dict()}, \n","        os.path.join(params['save_dir'], f'model_epoch_{epoch}.pth'))\n","  \n","  # Tensorboard\n","  writer.flush()\n","# Close tensorboard\n","writer.close()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"training_deraining_code.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}